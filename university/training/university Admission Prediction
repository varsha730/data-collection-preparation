import pandas as pd
import numpy as np
import pickle
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns
import sklearn
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
#read_csv is a pandas function to read csv files
data = pd.read_csv('Admission_Predict.csv')
data.head()
data.drop(["Serial No."],axis=1,inplace=True)
data.head
data.describe()
data.info()
data=data.rename(columns = {'chance od Admit': 'Chance of Admit'})
data.isnull().any()
data.corr()
plt.figure(figsize=(10,7))
sns.heatmap(data.corr(),annot=True,camp="RdYlGn")
sns.pairplot(data=data,hue='Research',makers=["^","v"],palette='inferno')
sns.scatterplot(X='University Rating',y='CGPA',data=data,color='Red', s=100)
category=['GRE Score','TOEFL Score','University Rating','SOP','LOR ','CGPA','Research','Chance of Admit ']
color=['yellowgreen','gold','lightskyblue','pink','red','purple','orange','yellow']
start=True 
for i in np.arange(4):
  fig=plt.figure(figsize=(14,8))
  plt.subplot2grid((4,2),(i,0))
  data[category[2*i]].hist(color=color[2*i],bins=10)
  plt.title(category[2*i])
  plt.subplot2grid((4,2),(i,1))
  data[category[2*i+1]].hist(color=color[2*i+1],bins=10)
  plt.title(category[2*i+1])

plt.subplots_adjust(hspace=0.7, wspace= 0.2)
plt.show()
data.head()
x=data.iloc[:,0,-1].values
x
y=data['Chance of Admit'].values
y
from sklearn.preprocessing import MinMaxScaler
sc = MinMaxScaler()
x=sc.fit_transform(x)
x
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.20,random_state=42)
#random_state actsas the seed for the random number generator during the split
y_train.shape
x_train
y_train=(y_train>0.5)
y_train
y_test=(y_test>0.5)
y_test
#model building - Logistic Regression
def logreg(x_train,x_test,y_train,y_test):
    lr = LogisticRegression(random_state=0)
    lr.fit(x_train,y_train)
    y_lr_tr = lr.predict(x_train)
    print(accuracy_score(y_lr_tr,y_train))
    ypred_lr = lr.predict(x_test)
    print(accuracy_score(ypred_lr,y_test))
    print("***Logistic Regression***")
    print("Confusion_Matrix")
    print(confusion_matrix(y_test,y_pred_lr))
    print("Classification Report")
    print(classification_report(y_test,y_pred_lr))
#printing the train accuracy and test accuracy respectively
logreg(x_train,x_test,y_train,y_test)
#testing on test & random input values
lr = LogisticRegression(random_state=0)
lr.fit(x_train,y_train)
print("Predicting on the test values")
lr_Pred = lr.Predict(x_test)
print("Output is: ",lr_pred)
print("Prediction on random input")
lr_pred_own = lr.predict(sc.transform([[337,118,4,4,5,4,5,9.65,1]]))
print("Output is: ",lr_pred_own)
#model building - Decision Tree classifier
def decisionTree(x_train,x_test,y_train,y_test):
    dtc = DecisionTreeClassifier(criterion="entropy",random_state=0)
    dtc.fit(x_train,y_train)
    y_dt_tr = dtc.predict(x_train)
    print(accuracy_score(y_dt_tr,y_train))
    yPred_dt = dtc.predict(x_test)
    print(accuracy_score(yPred_dt,y_test))
    print("***Decision Tree***")
    print("confusion_matrix")
    print(confusion_matrix(y_test,ypred_dt))
    print("classification Report")
    print(classification_Report(y_test,y_Pred_dt))
#printing the train accuracy and test accuracy respectively
decisionTree(x_train,x_test,y_train,y_test)
#testing on test & random inputvalues
dtc = DecisionTreeClassifier(criterion="entropy",random_state=0)
dtc.fit(x_train,y_train)
print("Prediction on test values")
dtc_Pred =dtc.predict(x_test)
print("output is: ",dtc_Pred)
print("Predicting on random input")
dtc_pred_own = dtc.predict(sc.transform([[337,118,4,4,5,4,5,9.65,1]]))
print("Output is: ",dtc_pred_own)

#modelbuilding - Random Forest Classifier
def RandomForest(x_train,x_test,y_train,y_test):
          rf = RandomForestClassifier(criterion="entropy",n_estimators=10,random_state=0)
          rf.fit(x_train,y_train)
          y_rf_tr = rf.predict(x_train)
          print(accuracy_score(yPred_rf,y_test))
          print("***Random Forest***")
          print("confusion_matrix")
          print(confusion_matrix(y_test,ypred_rf))
          print("classification Report")
          print(classification_Report(y_test,y_Pred_rf))
RandomForest(x_train,x_test,y_train,y_test)
#testing on test & random input values
rf = RandomForestClassifier(criterion="entropy", n_estimators=10, random_state=0)
rf.fit(x_train,y_train)
print("Predicting on test values")
rf_pred =rf.predict(x_test)
print("Output is: ",rf_pred)
print("Predicting on random input")
rf_pred_own = rf.predict(sc.transform([[337,118,4,4,5,4,5,9.65,1]]))
print("output is: ",rf_pred_own)

          
# Importing the keras liabarires and packages
import keras
from keras.models import Sequential
from keras.layers import Dense

classifier = Sequential()
classifier.add(Dense(units=7, activation='relu', input_dim=7))
classifier.add(Dense(units=7, activation='relu'))
classifier.add(Dense(units=1, activation='linear'))
classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model = classifier.fit(x_train, y_train, batch_size=10, validation_split=0.33, epochs=20)
ann_pred = classifier.predict(x_test)
ann_pred = (ann_pred>0.5)
print(accuracy_score(ann_pred,y_test))
print("***ANN MODEL***")
print("confusion_matrix")
print(confusion_matrix(y_test,ann_pred))
print("classification Report")
print(classification_Report(y_test,ann_pred))

print("Predicting on test input")
ann_pred = classifier.Predict(x_test)
ann_pred = (ann_pred>0.5)
print("Output is: ",ann_pred)
print("Predicting on random input")
ann_pred_own = classifier.predict(sc.transform([[337,118,4,4,5,4,5,9.65,1]]))
ann_pred_own = (ann_pred_own>0.5)
print("Output is: ",ann_pred_own)

pickle.dump(lr.open('university.pkl','wb'))
          
          
          